{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install binance-connector\n",
    "# pyspark==3.4.1\n",
    "# pydeequ==1.0.1\n",
    "# python-dotenv==1.0.0\n",
    "# nltk==3.8.1\n",
    "# apache-flink==1.17.1\n",
    "# kafka-python==2.0.2\n",
    "# pip install minio==7.1.16\n",
    "# pip install deltalake==0.10.2\n",
    "# pip install delta-spark==3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = open(\"/home/longnv95/Coding/MLOPs/api_binance.txt\", \"r\").read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB Password: airflow\n",
      "MinIO Key: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Method 1: Using python-dotenv (most common approach)\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()  # By default looks for .env in current directory\n",
    "# Or specify a path: load_dotenv('/path/to/your/.env')\n",
    "\n",
    "# Access environment variables\n",
    "db_password = os.getenv('POSTGRES_PASSWORD')\n",
    "minio_key = os.getenv('MINIO_ACCESS_KEY')\n",
    "fernet_key = os.getenv('AIRFLOW__CORE__FERNET_KEY')\n",
    "\n",
    "print(f\"DB Password: {db_password}\")\n",
    "print(f\"MinIO Key: {minio_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load API Key and run_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 00:00:00 1740502800000\n",
      "2025-02-26 23:00:00 1740585600000\n"
     ]
    }
   ],
   "source": [
    "from binance.spot import Spot\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Binance API Key (Public API, no need for secret key)\n",
    "client = Spot(api_key=API_KEY)  # No secret key needed for public market data\n",
    "\n",
    "# Define the time range for historical data (e.g. yesterday at noon)\n",
    "run_date_str = \"2025-02-26\"\n",
    "\n",
    "run_date = datetime.datetime.strptime(run_date_str, \"%Y-%m-%d\")\n",
    "run_date = datetime.datetime.combine(run_date, datetime.time(0, 0))\n",
    "start_time_ms = int(run_date.timestamp() * 1000)\n",
    "print(run_date, start_time_ms)\n",
    "\n",
    "end_date = run_date + datetime.timedelta(hours=23)\n",
    "end_time_ms = int(end_date.timestamp() * 1000)\n",
    "print(end_date, end_time_ms)\n",
    "\n",
    "# For a 24-hour period at 30-minute intervals, we expect 48 candles (24 hours / 0.5 hour)\n",
    "limit = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the result from Binance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BTCUSDT\n",
      "Processing ETHUSDT\n"
     ]
    }
   ],
   "source": [
    "# Retrieve exchange info to get a list of all trading symbols\n",
    "exchange_info = client.exchange_info()\n",
    "symbols = [s['symbol'] for s in exchange_info['symbols'] if s['symbol'].lower().endswith('usdt')]\n",
    "\n",
    "len(symbols)\n",
    "\n",
    "# List to collect aggregated data for each symbol\n",
    "data = []\n",
    "\n",
    "for symbol in symbols[0:2]:\n",
    "    print(f\"Processing {symbol}\")\n",
    "    try:\n",
    "        # Fetch klines data for the given symbol and day\n",
    "        klines = client.klines(symbol=symbol, interval=\"1h\", \n",
    "                                startTime=start_time_ms, \n",
    "                                endTime=end_time_ms,\n",
    "                                limit=limit)\n",
    "        if not klines:\n",
    "            print(f\"No data for {symbol}\")\n",
    "            continue\n",
    "        \n",
    "        for candle in klines:\n",
    "            # Calculate aggregated metrics from the klines\n",
    "            lastPrice = float(candle[4])  # Last candle's close price\n",
    "            highPrice = float(candle[2])  # Highest high price across candles\n",
    "            lowPrice = float(candle[3])  # Lowest low price across candles\n",
    "            quoteVolume = float(candle[7]) # Sum of quote asset volumes\n",
    "            count = int(candle[8])           # Total number of trades\n",
    "        \n",
    "            # Use the provided run date and extract hour from the close time of the last candle\n",
    "            close_time_ms = int(candle[6])\n",
    "            close_time = datetime.datetime.fromtimestamp(close_time_ms / 1000)\n",
    "            hour = close_time.hour\n",
    "\n",
    "            # Append the aggregated data to our list\n",
    "            data.append({\n",
    "                \"symbols\": symbol,\n",
    "                \"lastPrice\": lastPrice,\n",
    "                \"highPrice\": highPrice,\n",
    "                \"lowPrice\": lowPrice,\n",
    "                \"quoteVolume\": quoteVolume,\n",
    "                \"count\": count,\n",
    "                \"date\": close_time.strftime(\"%Y-%m-%d\"),\n",
    "                \"hour\": close_time.hour\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {symbol}: {e}\")\n",
    "\n",
    "    # Create the DataFrame with the desired columns\n",
    "    df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from binance.spot import Spot\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "API_KEY = open(\"/home/longnv95/Coding/MLOPs/api_binance.txt\", \"r\").read().strip()\n",
    "RUN_DATE_STR = \"2025-03-04\"\n",
    "\n",
    "client = Spot(api_key=API_KEY)\n",
    "\n",
    "def get_data_binance(symbol, start_date_str):\n",
    "    start_date = datetime.datetime.strptime(start_date_str, \"%Y-%m-%d\") \n",
    "    end_date = start_date + datetime.timedelta(hours=23)\n",
    "\n",
    "    start_time_ms = int(start_date.timestamp() * 1000)\n",
    "    end_time_ms = int(end_date.timestamp() * 1000)\n",
    "    \n",
    "    data = client.klines(symbol=symbol, interval=\"1h\", startTime=start_time_ms, endTime=end_time_ms)\n",
    "    df = pd.DataFrame(data, columns=[\"timestamp\", \"open\", \"high\", \n",
    "                                     \"low\", \"close\", \"volume\", \"close_time\", \n",
    "                                     \"quote_av\", \"trades\", \"tb_base_av\", \"tb_quote_av\", \"ignore\"])\n",
    "\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\") \n",
    "    df[\"close_time\"] = pd.to_datetime(df[\"close_time\"], unit=\"ms\")\n",
    "    df[\"symbol\"] = symbol\n",
    "    df[\"hour\"] = df['close_time'].apply(lambda x: x.hour)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m exchange_info \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mexchange_info()\n\u001b[1;32m      2\u001b[0m symbols \u001b[38;5;241m=\u001b[39m [s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m exchange_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbols\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musdt\u001b[39m\u001b[38;5;124m'\u001b[39m)][:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mget_data_binance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBTCUSDT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRUN_DATE_STR\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m, in \u001b[0;36mget_data_binance\u001b[0;34m(symbol, start_date_str)\u001b[0m\n\u001b[1;32m     19\u001b[0m data \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mklines(symbol\u001b[38;5;241m=\u001b[39msymbol, interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1h\u001b[39m\u001b[38;5;124m\"\u001b[39m, startTime\u001b[38;5;241m=\u001b[39mstart_time_ms, endTime\u001b[38;5;241m=\u001b[39mend_time_ms)\n\u001b[1;32m     20\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     21\u001b[0m                                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     22\u001b[0m                                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquote_av\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrades\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtb_base_av\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtb_quote_av\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 24\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m], unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mms\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     25\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose_time\u001b[39m\u001b[38;5;124m\"\u001b[39m], unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m symbol\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pandas/core/frame.py:3980\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3977\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3978\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3979\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 3980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pandas/core/frame.py:4187\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4184\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(existing_piece, DataFrame):\n\u001b[1;32m   4185\u001b[0m             value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(value, (\u001b[38;5;28mlen\u001b[39m(existing_piece\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m-> 4187\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pandas/core/frame.py:4146\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis), key, value)\n\u001b[1;32m   4145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iset_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4148\u001b[0m \u001b[38;5;66;03m# check if we are modifying a copy\u001b[39;00m\n\u001b[1;32m   4149\u001b[0m \u001b[38;5;66;03m# try to set first as we want an invalid\u001b[39;00m\n\u001b[1;32m   4150\u001b[0m \u001b[38;5;66;03m# value exception to occur first\u001b[39;00m\n\u001b[1;32m   4151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pandas/core/frame.py:4136\u001b[0m, in \u001b[0;36mDataFrame._iset_item_mgr\u001b[0;34m(self, loc, value, inplace)\u001b[0m\n\u001b[1;32m   4132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_iset_item_mgr\u001b[39m(\n\u001b[1;32m   4133\u001b[0m     \u001b[38;5;28mself\u001b[39m, loc: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mslice\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray, value, inplace: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   4134\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4135\u001b[0m     \u001b[38;5;66;03m# when called from _set_item_mgr loc can be anything returned from get_loc\u001b[39;00m\n\u001b[0;32m-> 4136\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pandas/core/internals/managers.py:1268\u001b[0m, in \u001b[0;36mBlockManager.iset\u001b[0;34m(self, loc, value, inplace)\u001b[0m\n\u001b[1;32m   1266\u001b[0m     removed_blknos\u001b[38;5;241m.\u001b[39mappend(blkno_l)\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1268\u001b[0m     nb \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1269\u001b[0m     blocks_tup \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1270\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[:blkno_l] \u001b[38;5;241m+\u001b[39m (nb,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[blkno_l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :]\n\u001b[1;32m   1271\u001b[0m     )\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m blocks_tup\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pandas/core/internals/blocks.py:1922\u001b[0m, in \u001b[0;36mNumpyBlock.delete\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdelete\u001b[39m(\u001b[38;5;28mself\u001b[39m, loc) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Block:\n\u001b[1;32m   1921\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdelete(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues, loc, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1922\u001b[0m     mgr_locs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr_locs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(values, placement\u001b[38;5;241m=\u001b[39mmgr_locs, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim)\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pandas/_libs/internals.pyx:169\u001b[0m, in \u001b[0;36mpandas._libs.internals.BlockPlacement.delete\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<__array_function__ internals>:179\u001b[0m, in \u001b[0;36mdelete\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exchange_info = client.exchange_info()\n",
    "symbols = [s['symbol'] for s in exchange_info['symbols'] if s['symbol'].lower().endswith('usdt')][:2]\n",
    "test = get_data_binance('BTCUSDT', RUN_DATE_STR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"./data_daily/spot_{run_date_str.replace('-','')}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Limit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders Placed (Last Minute): None\n",
      "Request Weights Used (Last Minute): 6\n",
      "Order Weight Used (Last Minute): None\n"
     ]
    }
   ],
   "source": [
    "from binance.spot import Spot\n",
    "import requests\n",
    "import json\n",
    "\n",
    "client = Spot(api_key=API_KEY)\n",
    "\n",
    "try:\n",
    "    # Make a  API call\n",
    "    # Use the requests library to get headers\n",
    "    symbol = \"BTCUSDT\"\n",
    "    interval = \"1m\"  # 1-minute klines\n",
    "    limit = 10  # Number of klines to retrieve (adjust as needed)\n",
    "\n",
    "    url = \"https://api.binance.com/api/v3/klines\"  # Binance Kline API endpoint\n",
    "    params = {'symbol': symbol, 'interval': interval, 'limit': limit}\n",
    "    headers = {'X-MBX-APIKEY': API_KEY}  # Include the API key in the header\n",
    "\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "\n",
    "    # Access the headers from the response object\n",
    "    headers = response.headers\n",
    "\n",
    "    # Extract rate limit information\n",
    "    # These headers are the key to understanding your limits\n",
    "    order_count = headers.get('X-MBX-ORDER-COUNT-1M')\n",
    "    used_weight = headers.get('X-MBX-USED-WEIGHT-1M')\n",
    "    order_weight = headers.get('X-MBX-ORDER-WEIGHT-1M')\n",
    "\n",
    "    print(f\"Orders Placed (Last Minute): {order_count}\")\n",
    "    print(f\"Request Weights Used (Last Minute): {used_weight}\")\n",
    "    print(f\"Order Weight Used (Last Minute): {order_weight}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking rate limits: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       User ID  Transaction ID      Amount      Vendor          Sources  \\\n",
      "0  user_000069    294063303192  579.803149   Education  Current Account   \n",
      "1  user_000000    413456285542  128.222111  Restaurant  Current Account   \n",
      "2  user_000023    168523878031  508.797821   Education      Credit Card   \n",
      "3  user_000095    160894934119  381.033603      Travel       Debit Card   \n",
      "4  user_000043    327308630118  611.396373   Education  Current Account   \n",
      "\n",
      "                  Time  \n",
      "0  2025-03-09 06:55:22  \n",
      "1  2025-09-03 22:19:31  \n",
      "2  2025-05-29 14:11:48  \n",
      "3  2025-03-20 23:05:23  \n",
      "4  2025-03-29 20:35:56  \n"
     ]
    }
   ],
   "source": [
    "RUN_DATE_STR = \"2025-03-04\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_pseudo_data(RUN_DATE_STR, num_rows=10000, num_users=1000):\n",
    "    \"\"\"Generates a Pandas DataFrame with pseudo transaction data.\"\"\"\n",
    "\n",
    "    # User IDs\n",
    "    user_ids = [f\"user_{i:06d}\" for i in np.random.choice(num_users, size=num_rows)]\n",
    "\n",
    "    # Transaction IDs\n",
    "    transaction_ids = np.random.randint(10**11, 10**12 - 1, size=num_rows)\n",
    "\n",
    "    # Source\n",
    "    sources = np.random.choice([\"Current Account\", \"Credit Card\", \"Debit Card\"], \n",
    "                               size=num_rows, p=[0.6, 0.3, 0.1])\n",
    "\n",
    "    # Amounts\n",
    "    amounts = np.random.uniform(1.0, 1000.0, size=num_rows)\n",
    "\n",
    "    # Vendors\n",
    "    vendors = [\"Online Shopping\", \"Hospital\", \"Sport\", \"Grocery\", \"Restaurant\",\n",
    "               \"Travel\", \"Entertainment\", \"Electronics\", \"Home Improvement\",\n",
    "               \"Clothing\", \"Education\", \"Sending Out\", \"Utilities\", \"Other\"]\n",
    "    vendor_probabilities = np.random.dirichlet(np.ones(len(vendors)))\n",
    "    vendor_choices = np.random.choice(vendors, size=num_rows, p=vendor_probabilities)\n",
    "\n",
    "    # Times\n",
    "    start_date = datetime.strptime(RUN_DATE_STR, \"%Y-%m-%d\")\n",
    "    time_deltas = [timedelta(seconds=np.random.randint(0, 31536000)) for _ in range(num_rows)]  # Up to 1 year\n",
    "    times = [(start_date + delta).strftime('%Y-%m-%d %H:%M:%S') for delta in time_deltas]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"User ID\": user_ids,\n",
    "        \"Transaction ID\": transaction_ids,\n",
    "        \"Amount\": amounts,\n",
    "        \"Vendor\": vendor_choices,\n",
    "        \"Sources\": sources,\n",
    "        \"Time\": times\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Generate and print the data\n",
    "pseudo_df = generate_pseudo_data(RUN_DATE_STR, num_rows=10000, num_users=100)\n",
    "print(pseudo_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/longnv95/Coding/MLOPs/final_project/data_daily/'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()+'/data_daily/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Vendor</th>\n",
       "      <th>Sources</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_000028</td>\n",
       "      <td>502338847571</td>\n",
       "      <td>244.895708</td>\n",
       "      <td>Home Improvement</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>2025-09-29 22:19:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_000727</td>\n",
       "      <td>479508446004</td>\n",
       "      <td>466.927997</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>Current Account</td>\n",
       "      <td>2025-03-21 22:14:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_000547</td>\n",
       "      <td>113555903812</td>\n",
       "      <td>971.725657</td>\n",
       "      <td>Home Improvement</td>\n",
       "      <td>Current Account</td>\n",
       "      <td>2025-07-11 22:30:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_000028</td>\n",
       "      <td>864048690465</td>\n",
       "      <td>417.963380</td>\n",
       "      <td>Travel</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2025-01-30 06:28:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_000323</td>\n",
       "      <td>917408956397</td>\n",
       "      <td>555.758447</td>\n",
       "      <td>Sending Out</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2025-04-05 20:19:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       User ID  Transaction ID      Amount            Vendor          Sources  \\\n",
       "0  user_000028    502338847571  244.895708  Home Improvement       Debit Card   \n",
       "1  user_000727    479508446004  466.927997        Restaurant  Current Account   \n",
       "2  user_000547    113555903812  971.725657  Home Improvement  Current Account   \n",
       "3  user_000028    864048690465  417.963380            Travel      Credit Card   \n",
       "4  user_000323    917408956397  555.758447       Sending Out      Credit Card   \n",
       "\n",
       "                  Time  \n",
       "0  2025-09-29 22:19:21  \n",
       "1  2025-03-21 22:14:40  \n",
       "2  2025-07-11 22:30:57  \n",
       "3  2025-01-30 06:28:11  \n",
       "4  2025-04-05 20:19:02  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet('/home/longnv95/Coding/MLOPs/final_project/data_daily/transaction_2024-12-31.parquet').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id gender location occupation   day_start\n",
      "0  user_002562      M   City D      Other  2022-10-18\n",
      "1  user_002095      F   City B          F  2025-01-15\n",
      "2  user_003486      M   City C          F  2021-01-24\n",
      "3  user_004358      F   City A          F  2024-04-21\n",
      "4  user_003419      F   City B          M  2022-06-25\n"
     ]
    }
   ],
   "source": [
    "RUN_DATE_STR = \"2025-03-04\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_user_data(RUN_DATE_STR, num_rows=10000):\n",
    "    \"\"\"Generates a Pandas DataFrame with pseudo transaction data.\"\"\"\n",
    "\n",
    "    # User IDs\n",
    "    user_ids = [f\"user_{i:06d}\" for i in np.random.choice(num_rows, size=num_rows)]\n",
    "\n",
    "    # Transaction IDs\n",
    "    age = np.random.randint(22, 90, size=num_rows)\n",
    "\n",
    "    # Source\n",
    "    gender = np.random.choice([\"M\", \"F\", \"Other\"], \n",
    "                               size=num_rows, p=[0.5, 0.4, 0.1])\n",
    "\n",
    "    # Amounts\n",
    "    occupation = np.random.choice([\"M\", \"F\", \"Other\"], \n",
    "                               size=num_rows, p=[0.5, 0.4, 0.1])\n",
    "\n",
    "    # location\n",
    "    location = np.random.choice([\"City A\", \"City B\", \"City C\", \"City D\", \"City E\", \n",
    "                                 \"City F\", \"City G\", \"City H\", \"Unknown\"], \n",
    "                               size=num_rows, p=[0.1,0.12,0.08,0.1,0.15,0.05,0.1,0.1,0.2])\n",
    "    \n",
    "    # Day start\n",
    "    start = datetime(2020, 1, 1)\n",
    "    end = datetime(2025, 3, 1)\n",
    "    \n",
    "    # Calculate total days between start and end\n",
    "    days_between = (end - start).days\n",
    "    \n",
    "    # Generate random days and add to start date\n",
    "    random_days = np.random.randint(0, days_between, size=num_rows)\n",
    "    day_start = [(start + timedelta(days=int(x))).strftime('%Y-%m-%d') for x in random_days]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"user_id\": user_ids,\n",
    "        \"gender\": gender,\n",
    "        \"age\": age,\n",
    "        \"location\": location,\n",
    "        \"occupation\": occupation,\n",
    "        \"day_start\": day_start,\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Generate and print the data\n",
    "pseudo_df = generate_user_data(RUN_DATE_STR, num_rows=5000)\n",
    "print(pseudo_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "dotenv_path = os.path.join(\"./scripts/\", '.env')\n",
    "load_dotenv(dotenv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/longnv95/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pyspark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741367937.707676 1093992 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/07 17:19:05 WARN Utils: Your hostname, LongNV resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/03/07 17:19:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/07 17:19:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Data\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n",
    "# Columns\n",
    "columns = [\"language\",\"users_count\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data).toDF(*columns)\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+\n",
      "|language|users_count|subtract|\n",
      "+--------+-----------+--------+\n",
      "|    Java|      20000| 19000.0|\n",
      "|  Python|     100000| 99000.0|\n",
      "|   Scala|       3000|  2000.0|\n",
      "+--------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"subtract\", df[\"users_count\"] - 1000)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest data to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent message: {'symbol': 'BTCUSDT', 'lastPrice': 23000.0, 'highPrice': 23200.0, 'lowPrice': 22900.0, 'quoteVolume': 500000.0, 'count': 1500, 'date': '2025-02-26', 'hour': 0}\n",
      "Sent message: {'symbol': 'ETHUSDT', 'lastPrice': 1800.0, 'highPrice': 1820.0, 'lowPrice': 1785.0, 'quoteVolume': 300000.0, 'count': 900, 'date': '2025-02-26', 'hour': 0}\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# from kafka import KafkaProducer\n",
    "\n",
    "# # Example DataFrame (imagine this is populated with Binance data)\n",
    "# data = {\n",
    "#     \"symbol\": [\"BTCUSDT\", \"ETHUSDT\"],\n",
    "#     \"lastPrice\": [23000.0, 1800.0],\n",
    "#     \"highPrice\": [23200.0, 1820.0],\n",
    "#     \"lowPrice\": [22900.0, 1785.0],\n",
    "#     \"quoteVolume\": [500000.0, 300000.0],\n",
    "#     \"count\": [1500, 900],\n",
    "#     \"date\": [\"2025-02-26\", \"2025-02-26\"],\n",
    "#     \"hour\": [0, 0]\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Initialize the Kafka Producer (adjust bootstrap_servers as needed)\n",
    "# producer = KafkaProducer(\n",
    "#     bootstrap_servers=['localhost:9092'],\n",
    "#     value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "# )\n",
    "\n",
    "# topic = \"crypto_data\"\n",
    "\n",
    "# # Push each DataFrame row to Kafka as a JSON message\n",
    "# for _, row in df.iterrows():\n",
    "#     # Convert the row to a dictionary\n",
    "#     message = row.to_dict()\n",
    "#     producer.send(topic, message)\n",
    "#     print(\"Sent message:\", message)\n",
    "\n",
    "# # Flush to ensure all messages are delivered\n",
    "# producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 'crypto_data' created with retention.ms set to 172800000 (2 days)\n",
      "Data inserted into Kafka topic successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create retention policy with Kafka \n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "# Step 1: Create a Kafka Admin Client to manage topics\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=['localhost:9092'],  # Adjust as needed\n",
    "    client_id='admin'\n",
    ")\n",
    "\n",
    "# Define the topic name\n",
    "topic = \"crypto_data\"\n",
    "\n",
    "# Define retention period for 2 days in milliseconds (2 days = 2 * 24 * 60 * 60 * 1000)\n",
    "retention_ms = str(2 * 24 * 60 * 60 * 1000)  # \"172800000\"\n",
    "\n",
    "# Create a new topic with the specified retention policy\n",
    "new_topic = NewTopic(\n",
    "    name=topic,\n",
    "    num_partitions=1, # Modify this if you have multiple nodes\n",
    "    replication_factor=1, # Modify this if you have multiple nodes\n",
    "    topic_configs={\"retention.ms\": retention_ms}\n",
    ")\n",
    "\n",
    "# Try to create the topic (if it already exists, you'll get an exception)\n",
    "try:\n",
    "    admin_client.create_topics(new_topics=[new_topic], validate_only=False)\n",
    "    print(f\"Topic '{topic}' created with retention.ms set to {retention_ms} (2 days)\")\n",
    "except Exception as e:\n",
    "    print(f\"Topic creation issue (may already exist): {e}\")\n",
    "\n",
    "# Step 2: Create a Kafka Producer to send messages\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],  # Adjust with your broker addresses\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')  # Serialize messages as JSON\n",
    ")\n",
    "\n",
    "# Example data to send (this could be a row from your DataFrame, for instance)\n",
    "data = {\n",
    "    \"symbols\": \"BTCUSDT\",\n",
    "    \"lastPrice\": 20000.0,\n",
    "    \"highPrice\": 21000.0,\n",
    "    \"lowPrice\": 19500.0,\n",
    "    \"quoteVolume\": 500000.0,\n",
    "    \"count\": 1500,\n",
    "    \"date\": \"2025-02-26\",\n",
    "    \"hour\": 14\n",
    "}\n",
    "\n",
    "# Send the data to the Kafka topic\n",
    "producer.send(topic, data)\n",
    "producer.flush()  # Ensure all buffered messages are sent\n",
    "\n",
    "print(\"Data inserted into Kafka topic successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted topic: crypto_data\n"
     ]
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError, UnknownTopicOrPartitionError\n",
    "\n",
    "def delete_topic(bootstrap_servers='localhost:9092', topic_name='topic_name'):\n",
    "    \"\"\"\n",
    "    Delete a Kafka topic\n",
    "    Args:\n",
    "        bootstrap_servers: Kafka bootstrap servers (default: 'localhost:9092')\n",
    "        topic_name: Name of topic to delete (default: 'topic_name')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create admin client\n",
    "        admin_client = KafkaAdminClient(\n",
    "            bootstrap_servers=bootstrap_servers\n",
    "        )\n",
    "        \n",
    "        # Delete topic\n",
    "        admin_client.delete_topics(topics=[topic_name])\n",
    "        print(f\"Successfully deleted topic: {topic_name}\")\n",
    "        \n",
    "    except UnknownTopicOrPartitionError:\n",
    "        print(f\"Topic {topic_name} does not exist\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete topic {topic_name}. Error: {e}\")\n",
    "    finally:\n",
    "        admin_client.close()\n",
    "\n",
    "# Usage example\n",
    "# if __name__ == \"__main__\":\n",
    "    # delete_topic(topic_name=\"crypto_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Binace Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_test = pd.read_csv(\"./data_daily/binance_data_2024-03-04.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]\n",
      "INFO:kafka.conn:Probing node bootstrap-0 broker version\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Connection complete.\n",
      "INFO:kafka.conn:Broker version identified as 2.5.0\n",
      "INFO:kafka.conn:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup\n",
      "INFO:kafka.conn:<BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]\n",
      "INFO:kafka.conn:<BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Connection complete.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. \n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 100 messages\n",
      "INFO:__main__:Processed batch of 52 messages\n",
      "INFO:__main__:Successfully sent 8952 messages to topic crypto_data\n",
      "INFO:kafka.producer.kafka:Closing the Kafka producer with 9223372036.0 secs timeout.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=1 host=localhost:9092 <connected> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. \n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "import logging\n",
    "import json\n",
    "from kafka import KafkaProducer \n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def send_message_batch(producer, topic_name: str, messages: list, batch_size: int = 100):\n",
    "    \"\"\"Send a batch of messages to Kafka\"\"\"\n",
    "    futures = []\n",
    "    \n",
    "    try:\n",
    "        for i in range(0, len(messages), batch_size):\n",
    "            batch = messages[i:i + batch_size]\n",
    "            \n",
    "            # Send batch of messages\n",
    "            for message in batch:\n",
    "                future = producer.send(topic_name, message)\n",
    "                futures.append(future)\n",
    "            \n",
    "            # Wait for current batch to complete\n",
    "            producer.flush()\n",
    "            \n",
    "            # Check for any errors in the batch\n",
    "            for future in futures:\n",
    "                try:\n",
    "                    record_metadata = future.get(timeout=10)\n",
    "                    logger.debug(f\"Message sent to {record_metadata.topic}:{record_metadata.partition}:{record_metadata.offset}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to send message: {e}\")\n",
    "            \n",
    "            logger.info(f\"Processed batch of {len(batch)} messages\")\n",
    "            futures = []  # Clear futures for next batch\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in batch processing: {e}\")\n",
    "        raise\n",
    "\n",
    "# Modified ingest function\n",
    "def ingest_data_to_kafka(df, topic_name: str, bootstrap_servers='localhost:9092', batch_size: int = 100):\n",
    "    \"\"\"\n",
    "    Ingest DataFrame to Kafka with batching and error handling\n",
    "    Args:\n",
    "        df: DataFrame to ingest\n",
    "        topic_name: Kafka topic name\n",
    "        bootstrap_servers: Kafka bootstrap servers\n",
    "        batch_size: Number of messages per batch\n",
    "    \"\"\"\n",
    "    # Create a Kafka Admin Client to manage topics\n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=['localhost:9092'],  # Adjust as needed\n",
    "        client_id='admin'\n",
    "    )\n",
    "\n",
    "    # Define retention period for 2 days in milliseconds (2 days = 2 * 24 * 60 * 60 * 1000)\n",
    "    retention_ms = str(2 * 24 * 60 * 60 * 1000)  # \"172800000\"\n",
    "\n",
    "    # Create a new topic with the specified retention policy\n",
    "    new_topic = NewTopic(\n",
    "        name=topic_name,\n",
    "        num_partitions=1, # Modify this if you have multiple nodes\n",
    "        replication_factor=1, # Modify this if you have multiple nodes\n",
    "        topic_configs={\"retention.ms\": retention_ms}\n",
    "    )\n",
    "\n",
    "    # Try to create the topic (if it already exists, you'll get an exception)\n",
    "    try:\n",
    "        admin_client.create_topics(new_topics=[new_topic], validate_only=False)\n",
    "        print(f\"Topic '{topic}' created with retention.ms set to {retention_ms} (2 days)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Topic creation issue (may already exist): {e}\")\n",
    "\n",
    "    # Ingest data to Kafka\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=[bootstrap_servers],\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "        acks='all',\n",
    "        retries=3,\n",
    "        batch_size=16384,\n",
    "        linger_ms=100,\n",
    "        compression_type='gzip'  # Add compression\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Convert DataFrame to list of dictionaries more efficiently\n",
    "        messages = df.to_dict('records')\n",
    "        \n",
    "        # Send messages in batches\n",
    "        send_message_batch(producer, topic_name, messages, batch_size)\n",
    "        \n",
    "        logger.info(f\"Successfully sent {len(messages)} messages to topic {topic_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to ingest data: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        producer.close()\n",
    "\n",
    "# Usage example:\n",
    "ingest_data_to_kafka(df_test, \"crypto_data\", batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data with Flink (Not Use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_time</th>\n",
       "      <th>quote_av</th>\n",
       "      <th>trades</th>\n",
       "      <th>tb_base_av</th>\n",
       "      <th>tb_quote_av</th>\n",
       "      <th>ignore</th>\n",
       "      <th>symbol</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-03 17:00:00</td>\n",
       "      <td>62845.14</td>\n",
       "      <td>62989.00</td>\n",
       "      <td>62458.01</td>\n",
       "      <td>62570.01</td>\n",
       "      <td>1725.46165</td>\n",
       "      <td>2024-03-03 17:59:59.999</td>\n",
       "      <td>1.082620e+08</td>\n",
       "      <td>90105</td>\n",
       "      <td>891.18899</td>\n",
       "      <td>5.592146e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-03 18:00:00</td>\n",
       "      <td>62570.01</td>\n",
       "      <td>62863.68</td>\n",
       "      <td>62570.00</td>\n",
       "      <td>62811.10</td>\n",
       "      <td>957.69137</td>\n",
       "      <td>2024-03-03 18:59:59.999</td>\n",
       "      <td>6.007572e+07</td>\n",
       "      <td>60591</td>\n",
       "      <td>501.56021</td>\n",
       "      <td>3.146042e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-03 19:00:00</td>\n",
       "      <td>62811.10</td>\n",
       "      <td>62857.00</td>\n",
       "      <td>62653.89</td>\n",
       "      <td>62730.00</td>\n",
       "      <td>814.77449</td>\n",
       "      <td>2024-03-03 19:59:59.999</td>\n",
       "      <td>5.112155e+07</td>\n",
       "      <td>59993</td>\n",
       "      <td>384.98781</td>\n",
       "      <td>2.415434e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-03 20:00:00</td>\n",
       "      <td>62730.00</td>\n",
       "      <td>62859.99</td>\n",
       "      <td>62580.00</td>\n",
       "      <td>62757.99</td>\n",
       "      <td>843.07635</td>\n",
       "      <td>2024-03-03 20:59:59.999</td>\n",
       "      <td>5.288540e+07</td>\n",
       "      <td>51386</td>\n",
       "      <td>395.49832</td>\n",
       "      <td>2.480857e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-03 21:00:00</td>\n",
       "      <td>62758.00</td>\n",
       "      <td>62828.18</td>\n",
       "      <td>62623.76</td>\n",
       "      <td>62827.11</td>\n",
       "      <td>652.28143</td>\n",
       "      <td>2024-03-03 21:59:59.999</td>\n",
       "      <td>4.092035e+07</td>\n",
       "      <td>43335</td>\n",
       "      <td>319.10493</td>\n",
       "      <td>2.001938e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp      open      high       low     close      volume  \\\n",
       "0  2024-03-03 17:00:00  62845.14  62989.00  62458.01  62570.01  1725.46165   \n",
       "1  2024-03-03 18:00:00  62570.01  62863.68  62570.00  62811.10   957.69137   \n",
       "2  2024-03-03 19:00:00  62811.10  62857.00  62653.89  62730.00   814.77449   \n",
       "3  2024-03-03 20:00:00  62730.00  62859.99  62580.00  62757.99   843.07635   \n",
       "4  2024-03-03 21:00:00  62758.00  62828.18  62623.76  62827.11   652.28143   \n",
       "\n",
       "                close_time      quote_av  trades  tb_base_av   tb_quote_av  \\\n",
       "0  2024-03-03 17:59:59.999  1.082620e+08   90105   891.18899  5.592146e+07   \n",
       "1  2024-03-03 18:59:59.999  6.007572e+07   60591   501.56021  3.146042e+07   \n",
       "2  2024-03-03 19:59:59.999  5.112155e+07   59993   384.98781  2.415434e+07   \n",
       "3  2024-03-03 20:59:59.999  5.288540e+07   51386   395.49832  2.480857e+07   \n",
       "4  2024-03-03 21:59:59.999  4.092035e+07   43335   319.10493  2.001938e+07   \n",
       "\n",
       "   ignore   symbol  hour  \n",
       "0       0  BTCUSDT    17  \n",
       "1       0  BTCUSDT    18  \n",
       "2       0  BTCUSDT    19  \n",
       "3       0  BTCUSDT    20  \n",
       "4       0  BTCUSDT    21  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv(\"./data_daily/binance_data_2024-03-04.csv\")   \n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time',\n",
       "       'quote_av', 'trades', 'tb_base_av', 'tb_quote_av', 'ignore', 'symbol',\n",
       "       'hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "import kafka\n",
    "print(kafka.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from Kafka and write down to MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/15 19:23:38 WARN Utils: Your hostname, LongNV resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/03/15 19:23:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/longnv95/Applications/extracts/Miniconda3-py38_4.8.3/envs/ame/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/longnv95/.ivy2/cache\n",
      "The jars for the packages stored in: /home/longnv95/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2f2ca241-a581-402b-8202-e6c5735171b4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.261 in central\n",
      ":: resolution report :: resolve 2036ms :: artifacts dl 210ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.261 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 by [com.amazonaws#aws-java-sdk-bundle;1.12.261] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   1   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2f2ca241-a581-402b-8202-e6c5735171b4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/45ms)\n",
      "25/03/15 19:23:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/15 19:24:08 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o66.showString.\n: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)\n\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)\n\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)\n\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)\n\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)\n\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)\n\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchPartitionOffsets(KafkaOffsetReaderAdmin.scala:128)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.getOffsetRangesFromUnresolvedOffsets(KafkaOffsetReaderAdmin.scala:374)\n\tat org.apache.spark.sql.kafka010.KafkaRelation.buildScan(KafkaRelation.scala:67)\n\tat org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:345)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:476)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:162)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:162)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:175)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:175)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:168)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:221)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:266)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:235)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Convert the string date (agg_date) to a proper date type (specify the format if needed)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m json_df \u001b[38;5;241m=\u001b[39m json_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mto_date(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyy-MM-dd\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 48\u001b[0m \u001b[43mjson_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o66.showString.\n: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)\n\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)\n\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)\n\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)\n\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)\n\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)\n\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchPartitionOffsets(KafkaOffsetReaderAdmin.scala:128)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.getOffsetRangesFromUnresolvedOffsets(KafkaOffsetReaderAdmin.scala:374)\n\tat org.apache.spark.sql.kafka010.KafkaRelation.buildScan(KafkaRelation.scala:67)\n\tat org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:345)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:476)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:162)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:162)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:175)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:175)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:168)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:221)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:266)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:235)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# Create Spark session configured for MinIO (S3A)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Weekly_Monthly_Feature_Calculation\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\" # previosly 2.4.0\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.2,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.261\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9010\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio_access_key\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio_secret_key\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read streaming data from Kafka topic \"crypto_data\"\n",
    "raw_kafka_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"crypto_data_agg_test\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "# Define the schema for your JSON data (adjust field names and types as needed)\n",
    "schema = StructType([\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"price_daily\", DoubleType(), True),\n",
    "    StructField(\"volume_daily\", DoubleType(), True),\n",
    "    StructField(\"trades_daily\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Parse the JSON from the Kafka \"value\" column (which is binary)\n",
    "json_df = raw_kafka_df.select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Convert the string date (agg_date) to a proper date type (specify the format if needed)\n",
    "json_df = json_df.withColumn(\"date\", F.to_date(F.col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "json_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import load_cfg\n",
    "from minio import Minio\n",
    "\n",
    "CFG_FILE = \"./utils/config.yaml\"\n",
    "cfg = load_cfg(CFG_FILE)\n",
    "datalake_cfg = cfg[\"datalake\"]\n",
    "crypto_data_cfg = cfg[\"crypto_data\"]\n",
    "\n",
    "# Initialize MinIO client\n",
    "minio_client = Minio(\n",
    "    datalake_cfg['endpoint'],  # Your MinIO server endpoint\n",
    "    access_key=datalake_cfg['access_key'],\n",
    "    secret_key=datalake_cfg['secret_key'],\n",
    "    secure=False  # Set to True if using HTTPS\n",
    ")\n",
    "\n",
    "# Create bucket if it doesn't exist\n",
    "bucket = 'crypto-test'\n",
    "if not minio_client.bucket_exists(bucket):\n",
    "    minio_client.make_bucket(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 17:55:16 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/03/10 17:55:21 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/03/10 17:55:27 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write the weekly features to Delta on MinIO using streaming write\n",
    "# json_df.write \\\n",
    "#     .format(\"parquet\") \\\n",
    "#     .mode(\"overwrite\")\\\n",
    "#     .save(f\"s3a://{bucket}/delta/test_features\")\n",
    "\n",
    "# from pyspark.sql import Row\n",
    "# test_df = spark.createDataFrame([Row(id=1, value=\"test\")])\n",
    "# test_df.write.format(\"json\").save(\"s3a://crypto-test/test_connection\")\n",
    "json_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(f\"s3a://{bucket}/delta_test/test_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crypto-test'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/15 19:24:36 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/03/15 19:24:45 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+------------------+-------------+---------------+-------------------+-------------------+----------+\n",
      "|    User ID|Transaction ID|            Amount|       Vendor|        Sources|               Time|          timestamp|      date|\n",
      "+-----------+--------------+------------------+-------------+---------------+-------------------+-------------------+----------+\n",
      "|user_000899|  658830851347|  974.933538286743|    Education|Current Account|2025-03-14 16:30:53|2025-03-14 16:30:53|2025-03-14|\n",
      "|user_000719|  743653405057|257.11832424180943|Entertainment|Current Account|2025-03-14 16:12:50|2025-03-14 16:12:50|2025-03-14|\n",
      "|user_000866|  730320152514| 477.4738513432964|        Other|     Debit Card|2025-03-14 19:51:52|2025-03-14 19:51:52|2025-03-14|\n",
      "|user_000977|  620364528242|117.96175593527273|    Education|Current Account|2025-03-14 03:42:49|2025-03-14 03:42:49|2025-03-14|\n",
      "|user_000198|  228177746961| 802.4465173614635|Entertainment|Current Account|2025-03-14 14:53:00|2025-03-14 14:53:00|2025-03-14|\n",
      "|user_000310|  695950861426| 745.9428661651496|    Utilities|    Credit Card|2025-03-14 07:50:52|2025-03-14 07:50:52|2025-03-14|\n",
      "|user_000600|  664716926683| 822.7766457769394|        Other|     Debit Card|2025-03-14 08:39:56|2025-03-14 08:39:56|2025-03-14|\n",
      "|user_000955|  506315032067| 797.6128985032801|    Utilities|Current Account|2025-03-14 20:04:16|2025-03-14 20:04:16|2025-03-14|\n",
      "|user_000294|  892194070028|  461.937545022413|    Education|Current Account|2025-03-14 01:01:44|2025-03-14 01:01:44|2025-03-14|\n",
      "|user_000094|  703108226970| 730.0216607648656|    Education|Current Account|2025-03-14 22:46:35|2025-03-14 22:46:35|2025-03-14|\n",
      "|user_000353|  258730341205| 541.9039961986505|    Education|    Credit Card|2025-03-14 17:41:36|2025-03-14 17:41:36|2025-03-14|\n",
      "|user_000032|  523242568384| 592.2358843715004|Entertainment|    Credit Card|2025-03-14 14:05:43|2025-03-14 14:05:43|2025-03-14|\n",
      "|user_000086|  170367217471| 84.38906018710387|    Education|    Credit Card|2025-03-14 15:44:06|2025-03-14 15:44:06|2025-03-14|\n",
      "|user_000525|  364607317794|151.09649549374325|        Other|     Debit Card|2025-03-14 07:36:58|2025-03-14 07:36:58|2025-03-14|\n",
      "|user_000751|  642368852977| 344.9088202589604|        Other|     Debit Card|2025-03-14 08:32:18|2025-03-14 08:32:18|2025-03-14|\n",
      "|user_000406|  838569564903|475.22093567153894|    Education|Current Account|2025-03-14 22:39:09|2025-03-14 22:39:09|2025-03-14|\n",
      "|user_000397|  900986390801|507.21173783757393|        Other|Current Account|2025-03-14 07:24:00|2025-03-14 07:24:00|2025-03-14|\n",
      "|user_000011|  669297235883| 680.8820250658804|        Other|Current Account|2025-03-14 22:37:55|2025-03-14 22:37:55|2025-03-14|\n",
      "|user_000397|  477917020143| 261.4504802638021|    Education|Current Account|2025-03-14 01:47:04|2025-03-14 01:47:04|2025-03-14|\n",
      "|user_000963|  785237289529| 585.4401378376758|        Sport|Current Account|2025-03-14 03:09:56|2025-03-14 03:09:56|2025-03-14|\n",
      "+-----------+--------------+------------------+-------------+---------------+-------------------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df = spark.read.format(\"json\").load(\"s3a://crypto-test/test_connection\")\n",
    "df = spark.read.format(\"delta\").load(f\"s3a://transaction-data/delta_test/\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Available Checkpoints ===\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: s3a://crypto-test/delta_test/test_features/offsets\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: s3a://crypto-test/delta_test/test_features/offsets\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 30 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# List checkpoints\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Available Checkpoints ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m batch_ids \u001b[38;5;241m=\u001b[39m \u001b[43mlist_checkpoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Get info about the most recent checkpoint\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_ids:\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mlist_checkpoints\u001b[0;34m(checkpoint_dir)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlist_checkpoints\u001b[39m(checkpoint_dir):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# List all files in the checkpoint directory\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     checkpoint_files \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moffsets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Extract batch IDs from the files\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     batch_ids \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pyspark/rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1814\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: s3a://crypto-test/delta_test/test_features/offsets\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: s3a://crypto-test/delta_test/test_features/offsets\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 30 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# ====== 1. List all checkpoints for a streaming query ======\n",
    "def list_checkpoints(checkpoint_dir):\n",
    "    # List all files in the checkpoint directory\n",
    "    checkpoint_files = spark.sparkContext.textFile(os.path.join(checkpoint_dir, \"offsets\")).collect()\n",
    "    \n",
    "    # Extract batch IDs from the files\n",
    "    batch_ids = []\n",
    "    for file in checkpoint_files:\n",
    "        if \"batch-\" in file:\n",
    "            batch_id = file.split(\"batch-\")[1].split(\".\")[0]\n",
    "            batch_ids.append(int(batch_id))\n",
    "    \n",
    "    # Sort batch IDs to show most recent first\n",
    "    batch_ids.sort(reverse=True)\n",
    "    \n",
    "    print(f\"Found {len(batch_ids)} checkpoints\")\n",
    "    for i, batch_id in enumerate(batch_ids[:10]):  # Show only the 10 most recent\n",
    "        print(f\"Batch ID: {batch_id}\")\n",
    "    \n",
    "    return batch_ids\n",
    "\n",
    "# ====== 2. Get metadata about a specific checkpoint ======\n",
    "def get_checkpoint_info(checkpoint_dir, batch_id):\n",
    "    # Read the metadata JSON file for this batch\n",
    "    metadata_path = os.path.join(checkpoint_dir, f\"offsets/batch-{batch_id}.json\")\n",
    "    metadata_df = spark.read.json(metadata_path)\n",
    "    \n",
    "    # Display checkpoint details\n",
    "    metadata_df.show(truncate=False)\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "# ====== 3. Find the checkpoint location used by a Delta table ======\n",
    "def get_delta_table_checkpoint(delta_table_path):\n",
    "    # Load the Delta table\n",
    "    delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "    \n",
    "    # Get the transaction log and last checkpoint information\n",
    "    history = delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationParameters\")\n",
    "    \n",
    "    # Show recent operations\n",
    "    print(\"Recent Delta table operations:\")\n",
    "    history.orderBy(\"version\", ascending=False).show(5, truncate=False)\n",
    "    \n",
    "    # Find streaming write operations with checkpoint info\n",
    "    streaming_writes = history.filter(\"operation = 'STREAMING UPDATE' OR operation = 'WRITE'\")\n",
    "    \n",
    "    # Extract checkpoint location from operation parameters\n",
    "    if streaming_writes.count() > 0:\n",
    "        # The checkpoint location should be in the operationParameters\n",
    "        checkpoint_info = streaming_writes.first()\n",
    "        \n",
    "        # Parse the JSON operationParameters to extract checkpoint location\n",
    "        params = streaming_writes.select(\"operationParameters\").first()[0]\n",
    "        if \"checkpointLocation\" in params:\n",
    "            print(f\"Checkpoint location: {params['checkpointLocation']}\")\n",
    "            return params['checkpointLocation']\n",
    "        else:\n",
    "            print(\"No checkpoint location found in operation parameters\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No streaming writes found in the Delta table history\")\n",
    "        return None\n",
    "\n",
    "# ====== 4. Execute the functions ======\n",
    "checkpoint_path = f\"s3a://{bucket}/delta_test/test_features\"\n",
    "# List checkpoints\n",
    "print(\"=== Available Checkpoints ===\")\n",
    "batch_ids = list_checkpoints(checkpoint_path)\n",
    "\n",
    "# # Get info about the most recent checkpoint\n",
    "# if batch_ids:\n",
    "#     print(\"\\n=== Most Recent Checkpoint Info ===\")\n",
    "#     get_checkpoint_info(checkpoint_path, batch_ids[0])\n",
    "\n",
    "# # Find checkpoint location used by the Delta table\n",
    "# print(\"\\n=== Delta Table Checkpoint Information ===\")\n",
    "# table_checkpoint = get_delta_table_checkpoint(delta_table_path)\n",
    "\n",
    "# # ====== 5. Bonus: Get commit info from Delta table ======\n",
    "# print(\"\\n=== Delta Table Commit Information ===\")\n",
    "# spark.sql(f\"DESCRIBE HISTORY delta.`{delta_table_path}`\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import subprocess\n",
    "\n",
    "# Path to delete\n",
    "path_to_delete = \"s3a://crypto-test/delta/\"\n",
    "\n",
    "def delete_s3_path(path: str):\n",
    "    \"\"\"\n",
    "    Delete a path in S3 using the Hadoop FileSystem API\n",
    "    Args:\n",
    "    \"\"\"\n",
    "    # Get the Hadoop configuration\n",
    "    hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "\n",
    "    # Get the FileSystem for S3\n",
    "    fs = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        spark.sparkContext._jvm.java.net.URI.create(path), hadoop_conf\n",
    "    )\n",
    "\n",
    "    # Create path object\n",
    "    hadoop_path = spark.sparkContext._jvm.org.apache.hadoop.fs.Path(path)\n",
    "\n",
    "    # Delete the path (true means recursive deletion)\n",
    "    if fs.exists(hadoop_path):\n",
    "        fs.delete(hadoop_path, True)\n",
    "        print(f\"Successfully deleted {path}\")\n",
    "    else:\n",
    "        print(f\"Path {path} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate weekly features: average price, total volume, and total trades per week\n",
    "# weekly_df = json_df.groupBy(\"symbol\", weekofyear(\"agg_date\").alias(\"week_number\")) \\\n",
    "#     .agg(\n",
    "#         avg(\"avgPrice\").alias(\"weekly_avg_price\"),\n",
    "#         _sum(\"totalVolume\").alias(\"weekly_total_volume\"),\n",
    "#         _sum(\"totalTrades\").alias(\"weekly_total_trades\")\n",
    "#     )\n",
    "\n",
    "# # Similarly, calculate monthly features\n",
    "# monthly_df = json_df.groupBy(\"symbol\", month(\"agg_date\").alias(\"month\")) \\\n",
    "#     .agg(\n",
    "#         avg(\"avgPrice\").alias(\"monthly_avg_price\"),\n",
    "#         _sum(\"totalVolume\").alias(\"monthly_total_volume\"),\n",
    "#         _sum(\"totalTrades\").alias(\"monthly_total_trades\")\n",
    "#     )\n",
    "\n",
    "# # Write the weekly features to Delta on MinIO using streaming write with a checkpoint location\n",
    "# weekly_query = weekly_df.writeStream \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .option(\"checkpointLocation\", \"s3a://your-bucket/delta/weekly_checkpoint\") \\\n",
    "#     .start(\"s3a://your-bucket/delta/weekly_features\")\n",
    "\n",
    "# # Write the monthly features to Delta on MinIO using streaming write with a checkpoint location\n",
    "# monthly_query = monthly_df.writeStream \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .option(\"checkpointLocation\", \"s3a://your-bucket/delta/monthly_checkpoint\") \\\n",
    "#     .start(\"s3a://your-bucket/delta/monthly_features\")\n",
    "\n",
    "# spark.streams.awaitAnyTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "dotenv_path = os.path.join(\"./scripts/\", '.env')  # Assuming .env is in the same directory\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Debezium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully inserted 20000 rows into transaction_data table at 2025-03-02\n",
      "Table contains 20000 rows\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, types, MetaData, Table, Column, String, Float, text, Integer, Identity\n",
    "\n",
    "# Expect a run date (e.g., \"2025-03-20\") as the first command-line argument\n",
    "RUN_DATE_STR = \"2025-03-02\"\n",
    "\n",
    "def generate_pseudo_data(run_date_str, num_rows=20000, num_users=1000):\n",
    "    \"\"\"Generates a Pandas DataFrame with pseudo transaction data.\"\"\"\n",
    "    # Generate pseudo user IDs\n",
    "    user_ids = [f\"user_{i:06d}\" for i in np.random.choice(num_users, size=num_rows)]\n",
    "    \n",
    "    # Generate pseudo transaction IDs\n",
    "    transaction_ids = np.random.randint(10**11, 10**12 - 1, size=num_rows)\n",
    "    \n",
    "    # Choose sources for the transactions\n",
    "    sources = np.random.choice([\"Current Account\", \"Credit Card\", \"Debit Card\"],\n",
    "                              size=num_rows, p=[0.6, 0.3, 0.1])\n",
    "    \n",
    "    # Generate random amounts\n",
    "    amounts = np.random.uniform(1.0, 1000.0, size=num_rows)\n",
    "    \n",
    "    # Choose vendors from a list, with random probabilities\n",
    "    vendors = [\"Online Shopping\", \"Hospital\", \"Sport\", \"Grocery\", \"Restaurant\",\n",
    "               \"Travel\", \"Entertainment\", \"Electronics\", \"Home Improvement\",\n",
    "               \"Clothing\", \"Education\", \"Sending Out\", \"Utilities\", \"Other\"]\n",
    "    vendor_probabilities = np.random.dirichlet(np.ones(len(vendors)))\n",
    "    vendor_choices = np.random.choice(vendors, size=num_rows, p=vendor_probabilities)\n",
    "    \n",
    "    # Generate pseudo times based on the run date\n",
    "    start_date = dt.datetime.strptime(run_date_str, \"%Y-%m-%d\")\n",
    "    time_deltas = [dt.timedelta(seconds=np.random.randint(0, 31536000)) for _ in range(num_rows)]\n",
    "    times = [(start_date + delta).strftime('%Y-%m-%d %H:%M:%S') for delta in time_deltas]\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"User ID\": user_ids,\n",
    "        \"Transaction ID\": transaction_ids,\n",
    "        \"Amount\": amounts,\n",
    "        \"Vendor\": vendor_choices,\n",
    "        \"Sources\": sources,\n",
    "        \"Time\": times\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def insert_data_to_postgres(df, table_name=\"transaction_data\"):\n",
    "    \"\"\"\n",
    "    Inserts the DataFrame into a PostgreSQL table.\n",
    "    It uses SQLAlchemy to connect to PostgreSQL.\n",
    "    \"\"\"\n",
    "    # Use connection string from environment or default to the Airflow connection string\n",
    "    postgres_conn_str = \"postgresql+psycopg2://airflow:airflow@localhost/airflow\"\n",
    "    engine = create_engine(postgres_conn_str)\n",
    "    \n",
    "    try:\n",
    "        # Define table metadata with primary key\n",
    "        metadata = MetaData()\n",
    "        transaction_table = Table(\n",
    "            table_name, metadata,\n",
    "            Column(\"id\", Integer, Identity(), primary_key=True),  # Auto-incrementing primary key\n",
    "            Column(\"User ID\", String(255)),\n",
    "            Column(\"Transaction ID\", String(255)),\n",
    "            Column(\"Amount\", Float),\n",
    "            Column(\"Vendor\", String(255)),\n",
    "            Column(\"Sources\", String(255)),\n",
    "            Column(\"Time\", String(255)),\n",
    "            schema='public'\n",
    "        )\n",
    "        \n",
    "        # Create the table in the database\n",
    "        metadata.create_all(engine)\n",
    "        \n",
    "        # Insert data into the table using pandas to_sql\n",
    "        df.to_sql(\n",
    "            name=table_name,\n",
    "            con=engine,\n",
    "            if_exists='append',  # Use 'append' to add data to the existing table\n",
    "            index=False,\n",
    "            schema='public'\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully inserted {len(df)} rows into {table_name} table at {RUN_DATE_STR}\")\n",
    "\n",
    "        # Verify the table exists\n",
    "        with engine.connect() as connection:\n",
    "            query = text(f\"SELECT COUNT(*) FROM public.{table_name}\")\n",
    "            result = connection.execute(query)\n",
    "            count = result.scalar()\n",
    "            print(f\"Table contains {count} rows\")\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate pseudo transaction data\n",
    "    df = generate_pseudo_data(RUN_DATE_STR)\n",
    "    \n",
    "    # Insert the pseudo data into PostgreSQL\n",
    "    insert_data_to_postgres(df, table_name=\"transaction_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "def check_connector_exists():\n",
    "    try:\n",
    "        response = requests.get('http://localhost:8083/connectors/postgres-connector')\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return False\n",
    "\n",
    "def check_kafka_connect_health():\n",
    "    try:\n",
    "        response = requests.get('http://localhost:8083')\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False\n",
    "\n",
    "def register_connector():\n",
    "    # Wait for Kafka Connect to be ready\n",
    "    retries = 30\n",
    "    while retries > 0:\n",
    "        if check_kafka_connect_health():\n",
    "            break\n",
    "        print(\"Waiting for Kafka Connect to be ready...\")\n",
    "        time.sleep(5)\n",
    "        retries -= 1\n",
    "    \n",
    "    if retries == 0:\n",
    "        raise Exception(\"Kafka Connect is not available after waiting\")\n",
    "    \n",
    "    # Check if connector already exists\n",
    "    if check_connector_exists():\n",
    "        print(\"Connector already registered, checking status...\")\n",
    "        response = requests.get('http://localhost:8083/connectors/postgres-connector/status')\n",
    "        print(f\"Connector status: {response.json()}\")\n",
    "        sys.exit(0)\n",
    "    \n",
    "    # Load connector configuration\n",
    "    config_path = './docker_all/config/config_debezium.json'\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Config file not found at {config_path}\")\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        connector_config = json.load(f)\n",
    "    \n",
    "    try:\n",
    "        # Register connector\n",
    "        response = requests.post(\n",
    "            'http://localhost:8083/connectors',\n",
    "            headers={'Content-Type': 'application/json'},\n",
    "            data=json.dumps(connector_config)\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 201:\n",
    "            print(\"Connector registered successfully\")\n",
    "            # Check connector status\n",
    "            time.sleep(2)  # Wait for connector to start\n",
    "            status_response = requests.get('http://localhost:8083/connectors/postgres-connector/status')\n",
    "            print(f\"Connector status: {status_response.json()}\")\n",
    "        else:\n",
    "            print(f\"Failed to register connector: {response.text}\")\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        print(f\"Failed to connect to Kafka Connect: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    register_connector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_connector_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "from dotenv import load_dotenv\n",
    "from minio import Minio\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get run date from command line argument\n",
    "RUN_DATE_STR = sys.argv[1]\n",
    "RUN_DATE = dt.datetime.strptime(RUN_DATE_STR, \"%Y-%m-%d\")\n",
    "RUN_DATE_STR_7DAYS = (RUN_DATE - dt.timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Environment variables\n",
    "KAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\", \"kafka:29092\")\n",
    "CDC_TOPIC = \"dbserver1.airflow.transaction_data\"  # Adjust if your topic name is different\n",
    "POSTGRES_CONN_STR = os.getenv(\"POSTGRES_CONN_STR\", \"jdbc:postgresql://postgres:5432/airflow\")\n",
    "POSTGRES_USER = os.getenv(\"POSTGRES_USER\", \"airflow\")\n",
    "POSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\", \"airflow\")\n",
    "MINIO_ENDPOINT = os.getenv(\"S3_ENDPOINT\", \"minio:9000\")\n",
    "MINIO_ACCESS_KEY = os.getenv(\"S3_ACCESS_KEY\", \"minio_access_key\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"S3_SECRET_KEY\", \"minio_secret_key\")\n",
    "MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\", \"features\")\n",
    "\n",
    "def main():\n",
    "    logger.info(f\"Starting data processing for date range: {RUN_DATE_STR_7DAYS} to {RUN_DATE_STR}\")\n",
    "    \n",
    "    # Create Spark session configured for MinIO (S3A)\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Combined_CDC_PostgreSQL_Processing\") \\\n",
    "        .config(\"spark.jars.packages\", \n",
    "                \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "                \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.2,\"\n",
    "                \"com.amazonaws:aws-java-sdk-bundle:1.12.261,\"\n",
    "                \"org.postgresql:postgresql:42.5.1\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", f\"http://{MINIO_ENDPOINT}\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # 1. Process CDC events from Kafka\n",
    "    cdc_df = process_cdc_events(spark)\n",
    "    \n",
    "    # 2. Query PostgreSQL data directly (for data that might not have CDC events yet)\n",
    "    postgres_df = query_postgres_data(spark)\n",
    "    \n",
    "    # 3. Combine the data (union the dataframes)\n",
    "    combined_df = combine_data(cdc_df, postgres_df)\n",
    "    \n",
    "    # 4. Calculate features\n",
    "    features_df = calculate_features(combined_df)\n",
    "    \n",
    "    # 5. Write results to MinIO using Delta format\n",
    "    write_to_minio(features_df)\n",
    "    \n",
    "    spark.stop()\n",
    "    logger.info(\"Processing completed successfully\")\n",
    "\n",
    "def process_cdc_events(spark):\n",
    "    \"\"\"Process CDC events from Kafka\"\"\"\n",
    "    logger.info(f\"Reading CDC events from Kafka topic: {CDC_TOPIC}\")\n",
    "    \n",
    "    # Read data from Kafka CDC topic\n",
    "    raw_kafka_df = spark.read \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "        .option(\"subscribe\", CDC_TOPIC) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "    \n",
    "    # Define schema for Debezium CDC events\n",
    "    # Note: This schema is specific to Debezium's PostgreSQL connector output format\n",
    "    cdc_schema = StructType([\n",
    "        StructField(\"schema\", StringType(), True),\n",
    "        StructField(\"payload\", StructType([\n",
    "            StructField(\"after\", StringType(), True),  # Data after change\n",
    "            StructField(\"before\", StringType(), True), # Data before change (for updates/deletes)\n",
    "            StructField(\"op\", StringType(), True),     # Operation type (c=create, u=update, d=delete)\n",
    "            StructField(\"ts_ms\", DoubleType(), True)   # Timestamp of the change\n",
    "        ]), True)\n",
    "    ])\n",
    "    \n",
    "    # Parse the value column (which contains the CDC event JSON)\n",
    "    parsed_df = raw_kafka_df \\\n",
    "        .selectExpr(\"CAST(value AS STRING) as json_value\") \\\n",
    "        .select(F.from_json(F.col(\"json_value\"), cdc_schema).alias(\"cdc\")) \\\n",
    "        .select(\"cdc.payload.*\")\n",
    "    \n",
    "    # Define schema for the transaction data\n",
    "    transaction_schema = StructType([\n",
    "        StructField(\"User ID\", StringType(), True),\n",
    "        StructField(\"Transaction ID\", StringType(), True),\n",
    "        StructField(\"Amount\", DoubleType(), True),\n",
    "        StructField(\"Vendor\", StringType(), True),\n",
    "        StructField(\"Sources\", StringType(), True),\n",
    "        StructField(\"Time\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Extract and parse the 'after' data (which contains the actual transaction data)\n",
    "    # Filter to only include 'create' and 'update' operations (skip deletes)\n",
    "    transactions_df = parsed_df \\\n",
    "        .where(\"op IN ('c', 'u')\") \\\n",
    "        .select(\n",
    "            F.from_json(F.col(\"after\"), transaction_schema).alias(\"data\"),\n",
    "            F.col(\"ts_ms\").alias(\"cdc_timestamp\")\n",
    "        ) \\\n",
    "        .select(\"data.*\", \"cdc_timestamp\")\n",
    "    \n",
    "    # Convert timestamp string to proper timestamp and add date column\n",
    "    result_df = transactions_df \\\n",
    "        .withColumn(\"timestamp\", F.to_timestamp(F.col(\"Time\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "        .withColumn(\"date\", F.to_date(F.col(\"timestamp\"))) \\\n",
    "        .where(f\"date BETWEEN '{RUN_DATE_STR_7DAYS}' AND '{RUN_DATE_STR}'\") \\\n",
    "        .drop(\"cdc_timestamp\")  # Remove the CDC timestamp as we don't need it anymore\n",
    "    \n",
    "    logger.info(f\"Processed {result_df.count()} CDC events\")\n",
    "    return result_df\n",
    "\n",
    "def query_postgres_data(spark):\n",
    "    \"\"\"Query PostgreSQL data directly\"\"\"\n",
    "    logger.info(\"Querying PostgreSQL database directly\")\n",
    "    \n",
    "    # Read directly from PostgreSQL for recent data that might not be in CDC yet\n",
    "    postgres_df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", POSTGRES_CONN_STR) \\\n",
    "        .option(\"dbtable\", \"transaction_data\") \\\n",
    "        .option(\"user\", POSTGRES_USER) \\\n",
    "        .option(\"password\", POSTGRES_PASSWORD) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .load()\n",
    "    \n",
    "    # Convert timestamp and filter by date range\n",
    "    result_df = postgres_df \\\n",
    "        .withColumn(\"timestamp\", F.to_timestamp(F.col(\"Time\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "        .withColumn(\"date\", F.to_date(F.col(\"timestamp\"))) \\\n",
    "        .where(f\"date BETWEEN '{RUN_DATE_STR_7DAYS}' AND '{RUN_DATE_STR}'\")\n",
    "    \n",
    "    logger.info(f\"Retrieved {result_df.count()} records from PostgreSQL\")\n",
    "    return result_df\n",
    "\n",
    "def combine_data(cdc_df, postgres_df):\n",
    "    \"\"\"Combine CDC events and PostgreSQL data, removing duplicates\"\"\"\n",
    "    logger.info(\"Combining CDC events and PostgreSQL data\")\n",
    "    \n",
    "    # Union the dataframes\n",
    "    combined_df = cdc_df.unionByName(postgres_df)\n",
    "    \n",
    "    # Remove duplicates based on Transaction ID (keeping the latest record)\n",
    "    deduplicated_df = combined_df \\\n",
    "        .withColumn(\"row_num\", F.row_number().over(\n",
    "            F.window.partitionBy(\"Transaction ID\").orderBy(F.desc(\"timestamp\"))\n",
    "        )) \\\n",
    "        .where(\"row_num = 1\") \\\n",
    "        .drop(\"row_num\")\n",
    "    \n",
    "    logger.info(f\"Combined data has {deduplicated_df.count()} unique transactions after deduplication\")\n",
    "    return deduplicated_df\n",
    "\n",
    "def calculate_features(df):\n",
    "    \"\"\"Calculate features from the transaction data\"\"\"\n",
    "    logger.info(\"Calculating features from transaction data\")\n",
    "    \n",
    "    # Calculate weekly features (l1w = last 1 week)\n",
    "    time_window = \"l1w\"\n",
    "    features_df = df.groupBy(\"User ID\") \\\n",
    "        .agg(\n",
    "            F.count(\"Transaction ID\").alias(f\"num_transactions_{time_window}\"),\n",
    "            F.sum(\"Amount\").alias(f\"total_amount_{time_window}\"),\n",
    "            F.avg(\"Amount\").alias(f\"avg_amount_{time_window}\"),\n",
    "            F.min(\"Amount\").alias(f\"min_amount_{time_window}\"),\n",
    "            F.max(\"Amount\").alias(f\"max_amount_{time_window}\"),\n",
    "            F.countDistinct(\"Vendor\").alias(f\"num_vendors_{time_window}\"),\n",
    "            F.countDistinct(\"Sources\").alias(f\"num_sources_{time_window}\")\n",
    "        )\n",
    "    \n",
    "    # Add a timestamp for when these features were calculated\n",
    "    features_df = features_df.withColumn(\"feature_timestamp\", F.current_timestamp())\n",
    "    \n",
    "    logger.info(f\"Calculated features for {features_df.count()} users\")\n",
    "    return features_df\n",
    "\n",
    "def write_to_minio(df):\n",
    "    \"\"\"Write features to MinIO using Delta format\"\"\"\n",
    "    logger.info(f\"Writing features to MinIO bucket: {MINIO_BUCKET}\")\n",
    "    \n",
    "    # Ensure MinIO bucket exists\n",
    "    minio_client = Minio(\n",
    "        MINIO_ENDPOINT,\n",
    "        access_key=MINIO_ACCESS_KEY,\n",
    "        secret_key=MINIO_SECRET_KEY,\n",
    "        secure=False\n",
    "    )\n",
    "    \n",
    "    if not minio_client.bucket_exists(MINIO_BUCKET):\n",
    "        minio_client.make_bucket(MINIO_BUCKET)\n",
    "        logger.info(f\"Created new bucket: {MINIO_BUCKET}\")\n",
    "    \n",
    "    # Write to MinIO using Delta format\n",
    "    df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "        .option(\"delta.minReaderVersion\", \"2\") \\\n",
    "        .option(\"delta.minWriterVersion\", \"5\") \\\n",
    "        .save(f\"s3a://{MINIO_BUCKET}/features/date={RUN_DATE_STR}\")\n",
    "    \n",
    "    logger.info(f\"Successfully wrote features to s3a://{MINIO_BUCKET}/features/date={RUN_DATE_STR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "from dotenv import load_dotenv\n",
    "from minio import Minio\n",
    "\n",
    "# Get run date from command line argument\n",
    "# RUN_DATE_STR = sys.argv[1]\n",
    "# RUN_DATE = dt.datetime.strptime(RUN_DATE_STR, \"%Y-%m-%d\")\n",
    "# RUN_DATE_STR_7DAYS = (RUN_DATE - dt.timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "dotenv_path = os.path.join(\"./scripts/\", '.env') \n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Environment variables\n",
    "KAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\n",
    "CDC_TRANSACTION_TOPIC = os.getenv(\"CDC_TRANSACTION_TOPIC\")\n",
    "POSTGRES_CON_STR = os.getenv(\"POSTGRES_CON_STR\")\n",
    "POSTGRES_USER = os.getenv(\"POSTGRES_USER\")\n",
    "POSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "MINIO_ENDPOINT = os.getenv(\"S3_ENDPOINT\")\n",
    "MINIO_ACCESS_KEY = os.getenv(\"S3_ACCESS_KEY\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"S3_SECRET_KEY\")\n",
    "MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\")\n",
    "\n",
    "    \n",
    "# Create Spark session configured for MinIO (S3A)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Combined_CDC_PostgreSQL_Processing\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.2,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.261,\"\n",
    "            \"org.postgresql:postgresql:42.5.1\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", f\"http://{MINIO_ENDPOINT}\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/21 18:11:32 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+---+-------------+-----------+\n",
      "|before|               after|              source| op|        ts_ms|transaction|\n",
      "+------+--------------------+--------------------+---+-------------+-----------+\n",
      "|  null|{1, user_000377, ...|{2.3.4.Final, pos...|  r|1742498526222|       null|\n",
      "|  null|{2, user_000981, ...|{2.3.4.Final, pos...|  r|1742498526233|       null|\n",
      "|  null|{3, user_000836, ...|{2.3.4.Final, pos...|  r|1742498526234|       null|\n",
      "|  null|{4, user_000962, ...|{2.3.4.Final, pos...|  r|1742498526234|       null|\n",
      "|  null|{5, user_000640, ...|{2.3.4.Final, pos...|  r|1742498526234|       null|\n",
      "+------+--------------------+--------------------+---+-------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/21 18:11:33 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+--------------+-----------------+-----------+---------------+-------------------+-------------+\n",
      "| op| id|    User ID|Transaction ID|           Amount|     Vendor|        Sources|               Time|        ts_ms|\n",
      "+---+---+-----------+--------------+-----------------+-----------+---------------+-------------------+-------------+\n",
      "|  u|  4|user_000962|  457239087380| 174.213256648849|Sending Out|Current Account|2025-06-09 18:27:06|1742578440256|\n",
      "|  u| 48|user_000468|  155352729109|515.6645561652284|Sending Out|Current Account|2025-03-27 00:14:38|1742578440256|\n",
      "|  u| 51|user_000303|  732970514123| 950.034224384725|Sending Out|Current Account|2025-04-20 09:00:18|1742578440256|\n",
      "|  u| 81|user_000013|  863604161233| 868.166665533098|Sending Out|Current Account|2025-04-09 09:08:31|1742578440256|\n",
      "|  u| 85|user_000324|  619489722520| 317.590035743889|Sending Out|Current Account|2026-01-01 16:36:22|1742578440256|\n",
      "+---+---+-----------+--------------+-----------------+-----------+---------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_kafka_df = spark.read \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", CDC_TRANSACTION_TOPIC) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "\n",
    "# Define the nested schema for transaction data fields\n",
    "transaction_fields = [\n",
    "    T.StructField(\"id\", T.IntegerType(), False),\n",
    "    T.StructField(\"User ID\", T.StringType(), True),\n",
    "    T.StructField(\"Transaction ID\", T.StringType(), True),\n",
    "    T.StructField(\"Amount\", T.DoubleType(), True),\n",
    "    T.StructField(\"Vendor\", T.StringType(), True),\n",
    "    T.StructField(\"Sources\", T.StringType(), True),\n",
    "    T.StructField(\"Time\", T.StringType(), True)\n",
    "]\n",
    "\n",
    "# Define the nested schema for source\n",
    "source_fields = [\n",
    "    T.StructField(\"version\", T.StringType(), False),\n",
    "    T.StructField(\"connector\", T.StringType(), False),\n",
    "    T.StructField(\"name\", T.StringType(), False),\n",
    "    T.StructField(\"ts_ms\", T.LongType(), False),\n",
    "    T.StructField(\"snapshot\", T.StringType(), True),\n",
    "    T.StructField(\"db\", T.StringType(), False),\n",
    "    T.StructField(\"sequence\", T.StringType(), True),\n",
    "    T.StructField(\"schema\", T.StringType(), False),\n",
    "    T.StructField(\"table\", T.StringType(), False),\n",
    "    T.StructField(\"txId\", T.LongType(), True),\n",
    "    T.StructField(\"lsn\", T.LongType(), True),\n",
    "    T.StructField(\"xmin\", T.LongType(), True)\n",
    "]\n",
    "\n",
    "# Define the schema for transaction block\n",
    "transaction_block_fields = [\n",
    "    T.StructField(\"id\", T.StringType(), False),\n",
    "    T.StructField(\"total_order\", T.LongType(), False),\n",
    "    T.StructField(\"data_collection_order\", T.LongType(), False)\n",
    "]\n",
    "\n",
    "# Complete CDC schema\n",
    "cdc_schema = T.StructType([\n",
    "    T.StructField(\"schema\", T.StructType([\n",
    "        T.StructField(\"type\", T.StringType()),\n",
    "        T.StructField(\"fields\", T.ArrayType(T.StringType())),\n",
    "        T.StructField(\"optional\", T.BooleanType()),\n",
    "        T.StructField(\"name\", T.StringType()),\n",
    "        T.StructField(\"version\", T.IntegerType())\n",
    "    ])),\n",
    "    T.StructField(\"payload\", T.StructType([\n",
    "        T.StructField(\"before\", T.StructType(transaction_fields), True),\n",
    "        T.StructField(\"after\", T.StructType(transaction_fields), True), \n",
    "        T.StructField(\"source\", T.StructType(source_fields), False),\n",
    "        T.StructField(\"op\", T.StringType(), False),\n",
    "        T.StructField(\"ts_ms\", T.LongType(), True),\n",
    "        T.StructField(\"transaction\", T.StructType(transaction_block_fields), True)\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Parse the JSON data\n",
    "cdc_df = raw_kafka_df \\\n",
    "  .selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "  .withColumn(\"parsed_data\", F.from_json(F.col(\"json_str\"), cdc_schema)) \\\n",
    "  .select(\"parsed_data.payload.*\")\n",
    "\n",
    "# Now show the parsed data\n",
    "cdc_df.show(5)\n",
    "\n",
    "# To get just the transaction data after the change\n",
    "transaction_data = cdc_df.where('''op in ('c','u')''').select(\n",
    "    \"op\", \n",
    "    \"after.id\", \n",
    "    \"after.`User ID`\", \n",
    "    \"after.`Transaction ID`\", \n",
    "    \"after.Amount\", \n",
    "    \"after.Vendor\", \n",
    "    \"after.Sources\", \n",
    "    \"after.Time\",\n",
    "    \"source.ts_ms\"\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id\").orderBy(F.col(\"ts_ms\").desc())\n",
    "transaction_data = transaction_data.withColumn(\"rank\", F.row_number().over(w)) \\\n",
    "                        .filter(F.col(\"rank\") == 1) \\\n",
    "                        .drop(\"rank\")\n",
    "\n",
    "transaction_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", POSTGRES_CON_STR) \\\n",
    "    .option(\"dbtable\", \"transaction_data\") \\\n",
    "    .option(\"user\", POSTGRES_USER) \\\n",
    "    .option(\"password\", POSTGRES_PASSWORD) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "postgres_df = postgres_df.withColumn(\"timestamp\", F.to_timestamp(F.col(\"Time\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                        .withColumn(\"date\", F.to_date(F.col(\"timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------+-----------------+---------------+---------------+-------------------+-------------------+----------+\n",
      "| id|    User ID|Transaction ID|           Amount|         Vendor|        Sources|               Time|          timestamp|      date|\n",
      "+---+-----------+--------------+-----------------+---------------+---------------+-------------------+-------------------+----------+\n",
      "|  1|user_000377|  989600621541|458.3600134428063|      Education|    Credit Card|2025-05-27 01:14:33|2025-05-27 01:14:33|2025-05-27|\n",
      "|  2|user_000981|  344360205463|38.14437463217147|      Education|    Credit Card|2025-07-30 11:36:25|2025-07-30 11:36:25|2025-07-30|\n",
      "|  3|user_000836|  754588743418|988.4897927900346|      Education|Current Account|2025-04-21 18:01:59|2025-04-21 18:01:59|2025-04-21|\n",
      "|  5|user_000640|  413270206972|78.78715816071131|Online Shopping|    Credit Card|2025-03-24 10:52:26|2025-03-24 10:52:26|2025-03-24|\n",
      "|  6|user_000205|  754963731286|943.7920762607532|  Entertainment|Current Account|2025-11-21 13:54:16|2025-11-21 13:54:16|2025-11-21|\n",
      "+---+-----------+--------------+-----------------+---------------+---------------+-------------------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "postgres_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install feast==0.34.1 delta-spark==2.4.0 feast-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/24 16:21:28 WARN Utils: Your hostname, LongNV resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/03/24 16:21:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/longnv95/Applications/extracts/Miniconda3-py38_4.8.3/envs/ame/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/longnv95/.ivy2/cache\n",
      "The jars for the packages stored in: /home/longnv95/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e3e4bae7-1c7d-407a-8c40-1b09f6b58755;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.261 in central\n",
      "\tfound org.postgresql#postgresql;42.5.1 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      ":: resolution report :: resolve 1366ms :: artifacts dl 37ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.261 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.5.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 by [com.amazonaws#aws-java-sdk-bundle;1.12.261] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   20  |   0   |   0   |   1   ||   19  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e3e4bae7-1c7d-407a-8c40-1b09f6b58755\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 19 already retrieved (0kB/33ms)\n",
      "25/03/24 16:21:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "from dotenv import load_dotenv\n",
    "from minio import Minio\n",
    "\n",
    "# Get run date from command line argument\n",
    "# RUN_DATE_STR = sys.argv[1]\n",
    "# RUN_DATE = dt.datetime.strptime(RUN_DATE_STR, \"%Y-%m-%d\")\n",
    "# RUN_DATE_STR_7DAYS = (RUN_DATE - dt.timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "dotenv_path = os.path.join(\"./scripts/\", '.env') \n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Environment variables\n",
    "KAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\n",
    "CDC_TRANSACTION_TOPIC = os.getenv(\"CDC_TRANSACTION_TOPIC\")\n",
    "POSTGRES_CON_STR = os.getenv(\"POSTGRES_CON_STR\")\n",
    "POSTGRES_USER = os.getenv(\"POSTGRES_USER\")\n",
    "POSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "MINIO_ENDPOINT = os.getenv(\"S3_ENDPOINT\")\n",
    "MINIO_ACCESS_KEY = os.getenv(\"S3_ACCESS_KEY\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"S3_SECRET_KEY\")\n",
    "MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\")\n",
    "S3_ENDPOINT_LOCAL = os.getenv(\"S3_ENDPOINT_LOCAL\")\n",
    "\n",
    "    \n",
    "# Create Spark session configured for MinIO (S3A)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Combined_CDC_PostgreSQL_Processing\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.2,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.261,\"\n",
    "            \"org.postgresql:postgresql:42.5.1\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", f\"http://{S3_ENDPOINT_LOCAL}\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/24 16:21:40 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transaction_data = spark.read.format(\"delta\").load('s3a://transaction-data/features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/24 16:21:58 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------------------+------------------+------------------+-----------------+---------------+---------------+\n",
      "|    User ID|num_transactions_l1w|  total_amount_l1w|    avg_amount_l1w|    min_amount_l1w|   max_amount_l1w|num_vendors_l1w|num_sources_l1w|\n",
      "+-----------+--------------------+------------------+------------------+------------------+-----------------+---------------+---------------+\n",
      "|user_000577|                   2| 991.0696512544381|495.53482562721905|361.35423266149826|629.7154185929398|              2|              2|\n",
      "|user_000113|                   2|1096.2816446042389| 548.1408223021194| 129.9053398401846|966.3763047640543|              2|              1|\n",
      "|user_000066|                   4| 2821.186816639094| 705.2967041597735|262.00653776009557|990.0429033721248|              3|              2|\n",
      "|user_000708|                   1|  761.276068586942|  761.276068586942|  761.276068586942| 761.276068586942|              1|              1|\n",
      "|user_000289|                   2|1177.6633714588284| 588.8316857294142| 557.1961431727973|620.4672282860312|              2|              1|\n",
      "+-----------+--------------------+------------------+------------------+------------------+-----------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transaction_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data = transaction_data.withColumn(\"date\", F.lit('2025-03-02'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_definitions.py\n",
    "from datetime import timedelta\n",
    "from feast import Entity, Feature, FeatureView, Field\n",
    "from feast.infra.offline_stores.file_source import FileSource  # Updated import\n",
    "from feast.types import Float32, Int64, String\n",
    "\n",
    "# Define the customer entity\n",
    "customer = Entity(\n",
    "    name=\"customer\",\n",
    "    join_keys=[\"user_id\"],\n",
    "    description=\"Customer identifier\"\n",
    ")\n",
    "\n",
    "# Customer demographic features\n",
    "customer_source = FileSource(\n",
    "    path=\"s3a://transaction-data-user/demographic/\",\n",
    "    event_timestamp_column=None,\n",
    ")\n",
    "\n",
    "customer_features = FeatureView(\n",
    "    name=\"customer_demographics\",\n",
    "    entities=[customer],\n",
    "    ttl=timedelta(days=365),\n",
    "    schema=[\n",
    "        Field(name=\"user_id\", dtype=String),\n",
    "        Field(name=\"age\", dtype=Int64),\n",
    "        Field(name=\"gender\", dtype=String),\n",
    "        Field(name=\"location\", dtype=String),\n",
    "        Field(name=\"occupation\", dtype=String),\n",
    "    ],\n",
    "    source=customer_source,\n",
    "    online=False\n",
    ")\n",
    "\n",
    "# Transaction features\n",
    "transaction_source = FileSource(\n",
    "    path=\"s3a://transaction-data/features/\",\n",
    "    event_timestamp_column=\"date\",  # Assuming you have this column\n",
    ")\n",
    "\n",
    "transaction_features = FeatureView(\n",
    "    name=\"transaction_features\",\n",
    "    entities=[customer],\n",
    "    ttl=timedelta(days=30),\n",
    "    schema=[\n",
    "        Field(name=\"user_id\", dtype=String),\n",
    "        Field(name=\"num_transactions_1w\", dtype=Int64),\n",
    "        Field(name=\"total_amount_1w\", dtype=Float32),\n",
    "        Field(name=\"avg_amount_1w\", dtype=Float32),\n",
    "        Field(name=\"min_amount_1w\", dtype=Float32),\n",
    "        Field(name=\"max_amount_1w\", dtype=Float32),\n",
    "        Field(name=\"num_vendors_1w\", dtype=Int64),\n",
    "        Field(name=\"num_sources_1w\", dtype=Int64),\n",
    "    ],\n",
    "    source=transaction_source,\n",
    "    online=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install 'feast[aws]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Expected a local filesystem path, got a URI: 's3a://transaction-data-user/demographic/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m store \u001b[38;5;241m=\u001b[39m FeatureStore(repo_path\u001b[38;5;241m=\u001b[39mFEATURE_STORE_PATH)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Apply feature definitions\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcustomer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustomer_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransaction_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Materialize features\u001b[39;00m\n\u001b[1;32m     20\u001b[0m store\u001b[38;5;241m.\u001b[39mmaterialize_incremental(end_date\u001b[38;5;241m=\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow())\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/feast/usage.py:299\u001b[0m, in \u001b[0;36mlog_exceptions_and_usage.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mtraceback \u001b[38;5;241m=\u001b[39m _trace_to_log(traceback)\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m traceback:\n\u001b[0;32m--> 299\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/feast/usage.py:288\u001b[0m, in \u001b[0;36mlog_exceptions_and_usage.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m ctx\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mupdate(attrs)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mexception:\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;66;03m# exception was already recorded\u001b[39;00m\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/feast/feature_store.py:893\u001b[0m, in \u001b[0;36mFeatureStore.apply\u001b[0;34m(self, objects, objects_to_delete, partial)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;66;03m# Validate all feature views and make inferences.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_all_feature_views(\n\u001b[1;32m    891\u001b[0m     views_to_update, odfvs_to_update, request_views_to_update, sfvs_to_update\n\u001b[1;32m    892\u001b[0m )\n\u001b[0;32m--> 893\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_inferences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_sources_to_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentities_to_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mviews_to_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43modfvs_to_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43msfvs_to_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservices_to_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# Add all objects to the registry and update the provider's infrastructure.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m data_sources_to_update:\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/feast/feature_store.py:586\u001b[0m, in \u001b[0;36mFeatureStore._make_inferences\u001b[0;34m(self, data_sources_to_update, entities_to_update, views_to_update, odfvs_to_update, sfvs_to_update, feature_services_to_update)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_inferences\u001b[39m(\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    578\u001b[0m     data_sources_to_update: List[DataSource],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    583\u001b[0m     feature_services_to_update: List[FeatureService],\n\u001b[1;32m    584\u001b[0m ):\n\u001b[1;32m    585\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Makes inferences for entities, feature views, odfvs, and feature services.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m     \u001b[43mupdate_data_sources_with_inferred_event_timestamp_col\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_sources_to_update\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m     update_data_sources_with_inferred_event_timestamp_col(\n\u001b[1;32m    591\u001b[0m         [view\u001b[38;5;241m.\u001b[39mbatch_source \u001b[38;5;28;01mfor\u001b[39;00m view \u001b[38;5;129;01min\u001b[39;00m views_to_update], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    592\u001b[0m     )\n\u001b[1;32m    594\u001b[0m     update_data_sources_with_inferred_event_timestamp_col(\n\u001b[1;32m    595\u001b[0m         [view\u001b[38;5;241m.\u001b[39mbatch_source \u001b[38;5;28;01mfor\u001b[39;00m view \u001b[38;5;129;01min\u001b[39;00m sfvs_to_update], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    596\u001b[0m     )\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/feast/inference.py:72\u001b[0m, in \u001b[0;36mupdate_data_sources_with_inferred_event_timestamp_col\u001b[0;34m(data_sources, config)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# loop through table columns to find singular match\u001b[39;00m\n\u001b[1;32m     68\u001b[0m timestamp_fields \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[1;32m     70\u001b[0m     col_name,\n\u001b[1;32m     71\u001b[0m     col_datatype,\n\u001b[0;32m---> 72\u001b[0m ) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata_source\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_table_column_names_and_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(ts_column_type_regex_pattern, col_datatype):\n\u001b[1;32m     74\u001b[0m         timestamp_fields\u001b[38;5;241m.\u001b[39mappend(col_name)\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/typeguard/__init__.py:1033\u001b[0m, in \u001b[0;36mtypechecked.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m memo \u001b[38;5;241m=\u001b[39m _CallMemo(python_func, _localns, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m   1032\u001b[0m check_argument_types(memo)\n\u001b[0;32m-> 1033\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     check_return_type(retval, memo)\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/feast/infra/offline_stores/file_source.py:161\u001b[0m, in \u001b[0;36mFileSource.get_table_column_names_and_types\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Adding support for different file format path\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# based on S3 filesystem\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filesystem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_legacy_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mschema\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(schema, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(schema, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;66;03m# Newer versions of pyarrow doesn't have this method,\u001b[39;00m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# but this field is good enough.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pyarrow/parquet/__init__.py:1663\u001b[0m, in \u001b[0;36mParquetDataset.__new__\u001b[0;34m(cls, path_or_paths, filesystem, schema, metadata, split_row_groups, validate_schema, filters, metadata_nthreads, read_dictionary, memory_map, buffer_size, partitioning, use_legacy_dataset, pre_buffer, coerce_int96_timestamp_unit)\u001b[0m\n\u001b[1;32m   1660\u001b[0m         use_legacy_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_legacy_dataset:\n\u001b[0;32m-> 1663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ParquetDatasetV2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# unsupported keywords\u001b[39;49;00m\n\u001b[1;32m   1673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_nthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_nthreads\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pyarrow/parquet/__init__.py:2329\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.__init__\u001b[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, **kwargs)\u001b[0m\n\u001b[1;32m   2327\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   2328\u001b[0m             filesystem \u001b[38;5;241m=\u001b[39m LocalFileSystem(use_mmap\u001b[38;5;241m=\u001b[39mmemory_map)\n\u001b[0;32m-> 2329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mis_file:\n\u001b[1;32m   2330\u001b[0m         single_file \u001b[38;5;241m=\u001b[39m path_or_paths\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pyarrow/_fs.pyx:441\u001b[0m, in \u001b[0;36mpyarrow._fs.FileSystem.get_file_info\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pyarrow/error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Applications/Miniconda/envs/ame/lib/python3.8/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Expected a local filesystem path, got a URI: 's3a://transaction-data-user/demographic/'"
     ]
    }
   ],
   "source": [
    "from feast import FeatureStore\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = os.path.join(\"./scripts/\", '.env') \n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Set up feature store path (remove trailing slash)\n",
    "FEATURE_STORE_PATH = \"/home/longnv95/Coding/MLOPs/final_project/feature_store\"\n",
    "\n",
    "# Initialize the feature store\n",
    "store = FeatureStore(repo_path=FEATURE_STORE_PATH)\n",
    "\n",
    "# Apply feature definitions\n",
    "store.apply([customer, customer_features, transaction_features])\n",
    "\n",
    "# Materialize features\n",
    "store.materialize_incremental(end_date=datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```yaml\n",
    "project: feature_store_demo\n",
    "registry: data/registry.db # Or registry: registry.db\n",
    "provider: local\n",
    "online_store:\n",
    "    type: sqlite\n",
    "    path: data/online_store.db # Path relative to feature_repo directory\n",
    "offline_store:\n",
    "  type: file\n",
    "  file_options:\n",
    "    file_url_scheme: s3a\n",
    "    s3_endpoint_url: http://{MINIO_ENDPOINT}\n",
    "    s3_access_key: {MINIO_ACCESS_KEY}\n",
    "    s3_secret_key: {MINIO_SECRET_KEY}\n",
    "entity_key_serialization_version: 2\n",
    "```\n",
    "\n",
    "```python\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "import os\n",
    "from feast import Entity, FeatureView, Field, FileSource, FeatureStore\n",
    "from feast.types import Float32, Int64\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "dotenv_path = os.path.join(\"/opt/airflow/external_scripts/\", '.env')  # Assuming .env is in the same directory\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# MinIO keys and access points\n",
    "MINIO_ENDPOINT = os.getenv(\"S3_ENDPOINT\")\n",
    "MINIO_ACCESS_KEY = os.getenv(\"S3_ACCESS_KEY\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"S3_SECRET_KEY\")\n",
    "MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\")\n",
    "\n",
    "# Path to your existing data in MinIO (adjust as needed)\n",
    "DATA_PATH = f\"s3a://{MINIO_BUCKET}/customer_features.parquet\"\n",
    "\n",
    "# Create feature repo directory\n",
    "repo_path = Path(\"feature_repo\")\n",
    "os.makedirs(repo_path, exist_ok=True)\n",
    "\n",
    "# Define entity\n",
    "customer = Entity(\n",
    "    name=\"customer\",\n",
    "    join_keys=[\"customer_id\"],\n",
    "    description=\"Customer identifier\"\n",
    ")\n",
    "\n",
    "# Define data source - pointing to your existing MinIO data\n",
    "customer_source = FileSource(\n",
    "    name=\"customer_source\",\n",
    "    path=DATA_PATH,\n",
    "    timestamp_field=\"event_timestamp\",\n",
    ")\n",
    "\n",
    "# Define feature view\n",
    "customer_features = FeatureView(\n",
    "    name=\"customer_features\",\n",
    "    entities=[customer],\n",
    "    schema=[\n",
    "        Field(name=\"daily_transactions\", dtype=Int64),\n",
    "        Field(name=\"total_spend\", dtype=Float32),\n",
    "        Field(name=\"average_basket_size\", dtype=Float32),\n",
    "    ],\n",
    "    source=customer_source,\n",
    "    ttl=timedelta(days=30),\n",
    ")\n",
    "\n",
    "fs = FeatureStore(repo_path=repo_path)\n",
    "\n",
    "# Apply feature definitions\n",
    "fs.apply([customer, customer_features])\n",
    "print(f\"Feature registry created at: {repo_path}/registry.db\")\n",
    "\n",
    "# ... (after fs.apply)\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Materializing latest features to online store...\")\n",
    "# Load the latest features for all entities up to the current time\n",
    "fs.materialize_incremental(end_date=datetime.now())\n",
    "print(\"Materialization complete.\")\n",
    "\n",
    "print(\"Feature store successfully set up with MinIO as the offline store and SQLite as the online store!\")\n",
    "```\n",
    "\n",
    "```python\n",
    "from feast import FeatureStore\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize feature store (only once)\n",
    "fs = FeatureStore(repo_path=Path(\"feature_repo\"))\n",
    "\n",
    "# Get online features for specific customers\n",
    "features = fs.get_online_features(\n",
    "    features=[\n",
    "        \"customer_features:daily_transactions\", \n",
    "        \"customer_features:total_spend\",\n",
    "        \"customer_features:average_basket_size\"\n",
    "    ],\n",
    "    entity_rows=[{\"customer_id\": 1}, {\"customer_id\": 2}]\n",
    ").to_dict()\n",
    "\n",
    "print(features)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
